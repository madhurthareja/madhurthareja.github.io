---
layout: about
title: About
permalink: /
subtitle: Multimodal AI × Embedded Systems Researcher

profile:
  align: right
  image: prof_pic.png
  image_circular: false
  more_info: >
    <p style="font-size: 12px">madhurthareja1105@gmail.com</p>
    <p style="font-size: 12px"><a href="https://github.com/madhurthareja" target="_blank">github.com/madhurthareja</a></p>
    <p style="font-size: 12px"><a href="https://www.linkedin.com/in/madhurthareja/" target="_blank">linkedin.com/in/madhurthareja</a></p>

news: false
selected_papers: false
social: false
---

I'm Madhur Thareja — a dual-major undergraduate pursuing the B.S. in Electronic Systems at IIT Madras and the B.Sc.(H) in Computer Science at BITS Pilani. I build end-to-end intelligent systems that connect sensing, representation, reasoning, and actuation, spanning computer vision, multimodal grounding, generative modeling, and embedded intelligence.

I currently split my research time between the University of Pittsburgh School of Medicine (LivecellX) and Carnegie Mellon University (MediLLM/MMedRAG), where I design multimodal reasoning pipelines for biomedical perception and diagnostic agents. Previously, at the AI Institute of South Carolina, I led the multi-agent architecture for PAL, integrating VLM semantic grounding with uncertainty-aware reinforcement learning for personalized tutoring.

On the hardware and systems side, I earned a national podium finish at Samsung Semiconductor's Chip Design Studio by crafting single-cycle and pipelined RISC-V microarchitectures and closing RTL-to-GDSII flows in Cadence Genus/Innovus. I also co-led the Autonomous Smart Wheelchair project at IIT Madras, architecting a fused LiDAR/stereo/GPS/IMU stack with distributed control, and I contribute to Intel's OpenVINO Toolkit for RISC-V RVV 1.0 backends.

My goal is to keep shrinking the gap between high-level reasoning and low-level actuation, enabling reliable autonomy in healthcare, mobility, and human-assistive settings. Feel free to reach out if you'd like to collaborate on multimodal agents, uncertainty-aware perception, or embedded deployments.

